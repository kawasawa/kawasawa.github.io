# Generated by GitHub Copilot

from pathlib import Path
from typing import List, Optional, Tuple

import numpy as np
from config import VectorSearchConfig
from entities import VectorCache
from logger import mcp_logger
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from text_search_engine import KnowledgeEntity, TextSearchEngine
from vector_cache_manager import VectorCacheManager

# NOTE: 埋め込みモデルとは文章や単語を意味ベースの数値ベクトルに変換するエンコーダを指す
#   入力テキストは埋め込みモデルによって数百から数千次元の数値ベクトルに変換される
#   意味的な特徴が空間上に表され、意味が近いほどベクトルも近くなる
EMBEDDING_MODELS = [
    "intfloat/multilingual-e5-small",  # 多言語対応の軽量モデル
    "sonoisa/sentence-bert-base-ja-mean-tokens",  # 日本語特化の軽量モデル
]


class CommentSearcher:
    """
    ワークスペース内のアノテーションコメントを検索・抽出・ベクトル化するための検索エンジン
    テキスト検索とベクトル検索の両方の機能を提供し、利用可能性に応じて自動的に使い分ける
    """

    def __init__(self, workspace_root: str, annotation_tag: str):
        """
        検索エンジンの初期化
        Args:
            workspace_root (str): ワークスペースのルートディレクトリ
            annotation_tag (str): 抽出対象のアノテーションコメントタグ
        """
        self.workspace_root = Path(workspace_root)
        self.annotation_tag = annotation_tag

        # テキスト検索エンジン
        self.text_search_engine = TextSearchEngine(self.workspace_root, annotation_tag)

        # ベクトル検索関連
        self.model: Optional[SentenceTransformer] = None
        self.vector_cache = VectorCache(knowledge_list=[], vectors=np.array([]))
        self.is_vector_ready = False

    def initialize_vector(self) -> bool:
        """
        ベクトル検索機能を初期化
        Returns:
            bool: 初期化が成功した場合True
        """

        # ベクトルが初期化済みの場合は何もしない
        if self.is_vector_ready:
            return True

        # 埋め込みモデルの初期化
        for model_name in EMBEDDING_MODELS:
            try:
                mcp_logger.info(f"埋め込みモデル {model_name} を初期化...")
                self.model = SentenceTransformer(model_name)
                break
            except Exception as e:
                mcp_logger.error(f"埋め込みモデル {model_name} の初期化に失敗: {e}")
        if self.model is None:
            mcp_logger.error("すべての埋め込みモデルの初期化に失敗しました")
            return False

        # コメントの抽出とベクトル化
        try:
            mcp_logger.info("コメントの収集を開始...")
            all_knowledge = self.text_search_engine.extract_all()
            mcp_logger.info("コメントの収集が完了しました")
            mcp_logger.info(f"収集されたコメント数: {len(all_knowledge)}")

            mcp_logger.info("ベクトルデータの構築を開始...")
            self._build_vector_index(all_knowledge)
            self.is_vector_ready = True
            mcp_logger.info("ベクトルデータの構築が完了しました")
            return True
        except Exception as e:
            mcp_logger.error(f"ベクトルデータの構築に失敗: {e}")
            return False

    def _build_vector_index(self, knowledge_list: List[KnowledgeEntity]) -> None:
        """
        ナレッジリストからベクトルインデックスを構築
        Args:
            knowledge_list (List[KnowledgeEntity]): ナレッジのリスト
        """

        # ナレッジが空の場合は空で初期化して終了
        if not knowledge_list:
            self.vector_cache = VectorCache(knowledge_list=[], vectors=np.array([]))
            return

        # 可能であればキャッシュを使用
        cache_manager = VectorCacheManager(self.workspace_root)
        if cache_manager.should_use_cache(knowledge_list):
            cached_data = cache_manager.load_from_cache()
            if cached_data:
                self.vector_cache = cached_data
                return

        # 情報源をベクトル化
        mcp_logger.info(f"ベクトル化を開始: {len(knowledge_list)} 件のコメント")
        texts = []
        for knowledge in knowledge_list:
            file_path = knowledge.file_path
            content = knowledge.content
            # ファイル情報とコメント内容を組み合わせて、より良いベクトル表現を作成
            text_for_embedding = f"ファイル: {file_path}\n{content}"
            texts.append(text_for_embedding)
        vectors = self.model.encode(texts, show_progress_bar=True)
        mcp_logger.info("ベクトル化が完了しました")

        # キャッシュに保存
        self.vector_cache = VectorCache(knowledge_list, vectors)
        cache_manager.save_to_cache(self.vector_cache)

    def search_knowledge(
        self,
        search_phrase: str,
        use_vector_search: bool = VectorSearchConfig.DEFAULT_USE_VECTOR_SEARCH,
        top_k: int = VectorSearchConfig.DEFAULT_TOP_K,
        similarity_threshold: float = VectorSearchConfig.DEFAULT_SIMILARITY_THRESHOLD,
    ) -> List[Tuple[KnowledgeEntity, Optional[float]]]:
        """
        コメントを検索
        Args:
            search_phrase (str): 検索フレーズ
            use_vector_search (bool): ベクトル検索を行うかどうか
            top_k (int): ベクトル検索で返す結果の最大数
            similarity_threshold (float): ベクトル検索での類似度閾値
        Returns:
            List[Tuple[KnowledgeEntity, Optional[float]]]: (ナレッジ, 類似度スコア) のリスト
                テキスト検索の場合、類似度スコアはNone
        """

        mcp_logger.debug(
            f"検索開始: 検索フレーズ {search_phrase}, ベクトル検索 {use_vector_search}, トップK {top_k}"
        )

        results = []
        if use_vector_search and self.is_vector_ready:
            results = self._vector_search(search_phrase, top_k, similarity_threshold)
        else:
            results = self._text_search(search_phrase)

        mcp_logger.debug(f"検索結果: {len(results)} 件")

        return results

    def _vector_search(
        self, search_phrase: str, top_k: int, similarity_threshold: float
    ) -> List[Tuple[KnowledgeEntity, float]]:
        """
        ベクトル検索を実行
        Args:
            search_phrase (str): 検索フレーズ
            top_k (int): 返す結果の最大数
            similarity_threshold (float): 類似度の閾値
        Returns:
            List[Tuple[KnowledgeEntity, float]]: (ナレッジ, 類似度スコア) のリスト
        """

        # ベクトルデータが無ければ空を返す
        if self.vector_cache.vectors is None or len(self.vector_cache.vectors) == 0:
            return []

        # クエリをベクトル化
        #   引数
        #       ["データベース接続の設定方法"]  # 検索語をリストで渡す
        #   戻り値
        #       [[0.1, 0.2, 0.3]]  # ベクトル化されたクエリ
        #
        # NOTE: ベクトル化された文章は意味的な特徴が数値で表現されている
        #   以下のような変換が行われる
        #     text = "データベース接続"
        #       ↓ 意味のある単位（トークン）に分割
        #     tokenization = ["データベース", "接続"]
        #       ↓ 意味ベースで数値化
        #     embedding = [0.1234, -0.5678, 0.9012, ..., 0.3456]  # 384次元ベクトル
        #   意味的に類似した文章は近いベクトルになる
        #     "DBに接続する"       - [0.2, 0.8, -0.1, ...]
        #     "データベースに繋ぐ" - [0.3, 0.7, -0.2, ...]  # 類似したベクトル
        #     "猫が走る"           - [0.9, -0.5, 0.8, ...]  # 異なるベクトル
        #   こうして計算されたベクトルの比較に用いられるのがコサイン類似度である
        #   コサイン類似度はベクトル間の角度を計算し、類似度を -1 (真逆) から 1 (完全一致) の範囲で表現する
        query_vector = self.model.encode([search_phrase])

        # コサイン類似度を計算
        #   引数
        #       [[0.1, 0.2, 0.3]],    # クエリ
        #       [
        #           [0.4, 0.5, 0.6],  # ナレッジ1
        #           [0.7, 0.8, 0.9],  # ナレッジ2
        #           [0.2, 0.3, 0.4]   # ナレッジ3
        #       ]
        #   戻り値
        #       [[0.85, 0.92, 0.78]]
        similarities = cosine_similarity(query_vector, self.vector_cache.vectors)[0]

        # 類似度の上位K件を取得
        #   引数
        #       [0.85, 0.92, 0.78]  # 類似度のリスト
        #   戻り値
        #       [1, 0, 2]           # 類似度の高い順に並べて際のインデックス
        sorted_indices = np.argsort(similarities)[::-1]
        results = []
        for i in sorted_indices[:top_k]:
            similarity = similarities[i]
            if similarity < similarity_threshold:
                break
            mcp_logger.debug(
                f"ベクトル検索: パス {self.vector_cache.knowledge_list[i].file_path}, 行 {self.vector_cache.knowledge_list[i].line_number}, 類似度 {similarity:.4f}"
            )
            results.append((self.vector_cache.knowledge_list[i], float(similarity)))
        return results

    def _text_search(self, search_phrase: str) -> List[Tuple[KnowledgeEntity, None]]:
        """
        全文検索を実行
        Args:
            search_phrase (str): 検索フレーズ
        Returns:
            List[Tuple[KnowledgeEntity, None]]: (ナレッジ, None) のリスト
        """

        results = []
        knowledge_list = self.text_search_engine.search(search_phrase)
        for knowledge in knowledge_list:
            mcp_logger.debug(
                f"全文検索: パス {knowledge.file_path}, 行 {knowledge.line_number}"
            )
            results.append((knowledge, None))
        return results

    def get_all_knowledge(self) -> List[KnowledgeEntity]:
        """
        すべてのアノテーションコメントを取得
        Returns:
            List[KnowledgeEntity]: すべてのナレッジ
        """
        if self.is_vector_ready:
            return self.vector_cache.knowledge_list
        else:
            # 実行時点での検索結果を返却（リアルタイム検索）
            return self.text_search_engine.extract_all()

    def clear_vector_cache(self) -> None:
        """
        ベクトル検索のキャッシュをクリア
        """
        cache_manager = VectorCacheManager(self.workspace_root)
        cache_manager.clear_cache()
